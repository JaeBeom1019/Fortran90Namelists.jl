"""
# module Tokenize



# Examples

```jldoctest
julia>
```
"""
module Tokenize

export Tokenizer, update_chars, lex, lex_name, lex_string, lex_numeric

const PUNCTUATION = raw"=+-*/\\()[]{},:;%&~<>?`|$#@"
const WHITESPACE = " \t\r\v\f"  # '\v' => '\x0b', '\f' => '\x0c' in Python

mutable struct Tokenizer
    characters
    prior_char::Union{Nothing,AbstractChar}
    char::Union{Nothing,AbstractChar}
    index::Int
    prior_delim::Union{Nothing,AbstractChar}
    group_token::Union{Nothing,AbstractChar}  # Set to true if inside a namelist group
end
function Tokenizer(;
    characters=nothing,
    prior_char=nothing,
    char=nothing,
    index=0,
    prior_delim=nothing,
    group_token=nothing,
)
    return Tokenizer(characters, prior_char, char, index, prior_delim, group_token)
end

"""
    update_chars(tk::Tokenizer)

Update the current charters in the tokenizer.
"""
function update_chars(tk::Tokenizer)
    tk.prior_char, tk.char = tk.char, next(tk.characters, '\n')
    return tk.index += 1
end

function lex(tk::Tokenizer, line)
    tokens = String[]
    tk.index = 0   # Bogus value to ensure idx = 1 after first iteration
    tk.characters = Iterators.Stateful(line)  # An iterator generated by `line`
    update_chars(tk)
    while tk.char != '\n'
        # Update namelist group status
        if occursin(tk.char, raw"&$")
            tk.group_token = tk.char
        end
        if !isnothing(tk.group_token) &&
            ((tk.group_token, tk.char) in (('&', '/'), ('$', '$')))
            # A namelist ends, the value cannot be the default value (`nothing`)
            # Because it is being compared below
            tk.group_token = '\0'
        end
        word = ""  # Create or clear `word`
        if occursin(tk.char, WHITESPACE)  # Ignore whitespace
            while occursin(tk.char, WHITESPACE)
                word *= tk.char  # Read one char to `word`
                update_chars(tk)  # Read the next char until meet a non-whitespace char
            end
        elseif occursin(tk.char, raw"!#") || isnothing(tk.group_token)  # Ignore comment
            # Abort the iteration and build the comment token
            word = line[(tk.index):end]  # There is no '\n' at line end, no worry! Lines are already separated at line ends
            tk.char = '\n'
        elseif tk.char in ('\'', '"') || !isnothing(tk.prior_delim)  # Lex a string
            word = lex_string(tk)
        elseif occursin(tk.char, PUNCTUATION)
            word = tk.char
            update_chars(tk)
        else
            while !(isspace(tk.char) || occursin(tk.char, PUNCTUATION))
                word *= tk.char
                update_chars(tk)
            end
        end
        push!(tokens, string(word))
    end
    return tokens
end

"""
    lex_name(tk::Tokenizer, line)

Tokenize a Fortran name, such as a variable or subroutine.
"""
function lex_name(tk::Tokenizer, line::AbstractString)
    endindex = tk.index
    for char in line[(tk.index):end]
        !isalnum(char) && !occursin(char, raw"'\"_") && break
        endindex += 1
    end

    word = line[(tk.index):(endindex - 1)]  # Do not include non-alphanumeric and non-'"_ characters in `word`

    tk.index = endindex - 1  # Do not include non-alphanumeric and non-'"_ characters
    # Update iterator, minus first character which was already read
    # Continue iterating from `length(word)` => drop the first `length(word) - 1` characters
    tk.characters = Iterators.Stateful(
        collect(Iterators.drop(tk.characters, length(word) - 1))
    )
    update_chars(tk)
    return word
end

"""
    lex_string(tk::Tokenizer)

Tokenize a Fortran string.
"""
function lex_string(tk::Tokenizer)
    word = ""

    if !isnothing(tk.prior_delim)  # A previous quotation mark presents
        delim = tk.prior_delim  # Read until `delim`
        tk.prior_delim = nothing
    else
        delim = tk.char  # No previous quotation mark presents
        word *= tk.char  # Read this character
        update_chars(tk)
    end

    while true
        if tk.char == delim
            # Check for escaped delimiters
            update_chars(tk)
            if tk.char == delim
                word *= repeat(delim, 2)
                update_chars(tk)
            else
                word *= delim
                break
            end
        elseif tk.char == '\n'
            tk.prior_delim = delim
            break
        else
            word *= tk.char
            update_chars(tk)
        end
    end

    return word
end

"""
    lex_numeric(tk::Tokenizer)

Tokenize a Fortran numerical value.
"""
function lex_numeric(tk::Tokenizer)
    word = ""
    frac = false

    if tk.char == '-'
        word *= tk.char  # `word == "-"``
        update_chars(tk)
    end

    # Read as long as `tk.char` is a digit, or not a dot
    while isdigit(tk.char) || (tk.char == '.' && !frac)
        # Only allow one decimal point
        if tk.char == '.'
            frac = true  # If meet '.', break the loop
        end
        word *= tk.char
        update_chars(tk)
    end

    # Check for float exponent
    if occursin(tk.char, "eEdD")
        word *= tk.char
        update_chars(tk)
    end

    if occursin(tk.char, "+-")
        word *= tk.char
        update_chars(tk)
    end

    while isdigit(tk.char)
        word *= tk.char
        update_chars(tk)
    end

    return word
end

isalnum(c) = isletter(c) || isnumeric(c)

function next(iterable, default)
    x = iterate(iterable)
    return isnothing(x) ? default : first(x)
end

end
